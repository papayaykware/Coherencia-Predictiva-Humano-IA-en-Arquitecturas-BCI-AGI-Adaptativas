# ğŸ§  CPEA â€” FASE 2

# ğŸ” ConstrucciÃ³n del Bucle Cognitivo EEGâ€“AGI

## Protocolo MatemÃ¡tico y EspecificaciÃ³n AlgorÃ­tmica Online

---

![Status](https://img.shields.io/badge/status-active_development-blue)
![Framework](https://img.shields.io/badge/framework-PyTorch-red)
![License](https://img.shields.io/badge/license-MIT-green)
![Version](https://img.shields.io/badge/version-2.0-critical_loop)
![DOI](https://img.shields.io/badge/DOI-10.5281%2Fzenodo.XXXXXXX-lightgrey)

---

> âš ï¸ **Documento tÃ©cnico avanzado**
> Este documento formaliza matemÃ¡ticamente la FASE 2 del proyecto CPEA: construcciÃ³n del bucle cognitivo adaptativo EEGâ€“AGI con coherencia predictiva bidireccional.

---

# ğŸ“š Tabla de Contenidos

* [1. IntroducciÃ³n](#1-introducciÃ³n)
* [2. FormalizaciÃ³n MatemÃ¡tica](#2-formalizaciÃ³n-matemÃ¡tica)

  * [2.1 NotaciÃ³n](#21-notaciÃ³n)
  * [2.2 Modelo Generativo](#22-modelo-generativo)
  * [2.3 FunciÃ³n de PÃ©rdida Compuesta](#23-funciÃ³n-de-pÃ©rdida-compuesta)
  * [2.4 MÃ©trica de Coherencia Predictiva](#24-mÃ©trica-de-coherencia-predictiva)
* [3. Arquitectura PyTorch](#3-arquitectura-pytorch)
* [4. MÃ³dulo de AdaptaciÃ³n Online](#4-mÃ³dulo-de-adaptaciÃ³n-online)
* [5. RÃ©gimen CrÃ­tico DinÃ¡mico](#5-rÃ©gimen-crÃ­tico-dinÃ¡mico)
* [6. Protocolo Experimental](#6-protocolo-experimental)
* [7. Notebooks Reproducibles](#7-notebooks-reproducibles)
* [8. Referencias](#8-referencias)

---

# 1ï¸âƒ£ IntroducciÃ³n

La FASE 2 del proyecto CPEA transforma un modelo predictivo unidireccional en un **sistema dinÃ¡mico de acoplamiento recÃ­proco**. El objetivo no es Ãºnicamente predecir EEG, sino establecer una reducciÃ³n estructurada y sostenida del error dinÃ¡mico bajo condiciones de no estacionariedad.

La coherencia predictiva se define como:

[
CP_t = 1 - \frac{H(X_{t+1} | Z_t)}{H(X_{t+1})}
]

donde ( Z_t ) representa el embedding latente del modelo artificial.

---

# 2ï¸âƒ£ FormalizaciÃ³n MatemÃ¡tica

## 2.1 NotaciÃ³n

* ( X_t \in \mathbb{R}^{C \times T} ): ventana EEG multicanal
* ( Z_t \in \mathbb{R}^{d} ): estado latente
* ( \theta ): parÃ¡metros del modelo
* ( \hat{X}_{t+1} ): predicciÃ³n

---

## 2.2 Modelo Generativo

[
Z_t = f_\theta(X_t)
]

[
\hat{X}*{t+1} = g*\theta(Z_t)
]

Modelo probabilÃ­stico:

[
p_\theta(X_{t+1} | Z_t) =
\mathcal{N}(\mu_\theta(Z_t), \Sigma_\theta(Z_t))
]

---

## 2.3 FunciÃ³n de PÃ©rdida Compuesta

[
\mathcal{L}*t =
\alpha \mathcal{L}*{pred}

* \beta \mathcal{L}_{spectral}
* \gamma \mathcal{L}_{entropy}
* \lambda \mathcal{L}_{stability}
  ]

### ğŸ”¹ PÃ©rdida predictiva

[
\mathcal{L}*{pred} =
||X*{t+1} - \hat{X}_{t+1}||^2
]

### ğŸ”¹ PÃ©rdida espectral

[
\mathcal{L}*{spectral} =
||FFT(X*{t+1}) - FFT(\hat{X}_{t+1})||^2
]

### ğŸ”¹ PenalizaciÃ³n de estabilidad latente

[
\mathcal{L}*{stability} =
||Z_t - Z*{t-1}||^2
]

---

## 2.4 MÃ©trica de Coherencia Predictiva

AproximaciÃ³n prÃ¡ctica:

[
CP_t \approx
1 - \frac{\mathcal{L}_{pred}}{\sigma_X^2}
]

Suavizado exponencial:

[
\bar{CP}*t =
\eta CP_t + (1-\eta)\bar{CP}*{t-1}
]

---

# 3ï¸âƒ£ Arquitectura PyTorch

> ğŸ’¡ RecomendaciÃ³n: Transformer causal + generador lineal probabilÃ­stico

```python
class EEGEncoder(nn.Module):
    def __init__(self, d_model, nhead, num_layers):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

    def forward(self, x):
        return self.transformer(x)

class Generator(nn.Module):
    def __init__(self, d_model, out_dim):
        super().__init__()
        self.fc_mu = nn.Linear(d_model, out_dim)
        self.fc_logvar = nn.Linear(d_model, out_dim)

    def forward(self, z):
        return self.fc_mu(z), self.fc_logvar(z)
```

---

# 4ï¸âƒ£ MÃ³dulo de AdaptaciÃ³n Online

## ActivaciÃ³n Condicional

[
\Delta CP_t = \bar{CP}*t - \bar{CP}*{t-1}
]

Si:

[
\Delta CP_t < -\delta
]

â†’ aumentar tasa de aprendizaje.

---

## Tasa dinÃ¡mica

[
\eta_t = \eta_0 (1 + k|\Delta CP_t|)
]

---

## RegularizaciÃ³n EWC

[
\mathcal{L}_{EWC} =
\sum_i F_i(\theta_i - \theta_i^*)^2
]

---

### ğŸ” Algoritmo Online

```python
for X_t in stream:

    Z_t = encoder(X_t)
    mu, logvar = generator(Z_t)

    loss = compute_loss(X_t_next, mu, logvar)

    CP = compute_coherence(loss)

    if CP_drop_detected:
        optimizer.param_groups[0]["lr"] *= 1.5

    loss.backward()
    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
    optimizer.step()
```

---

# 5ï¸âƒ£ RÃ©gimen CrÃ­tico DinÃ¡mico

EstimaciÃ³n tipo Lyapunov:

[
\lambda_{local} =
\frac{1}{\Delta t}
\log \frac{||Z_t - Z_{t-1}||}
{||Z_{t-1} - Z_{t-2}||}
]

Objetivo:

[
\lambda_{local} \approx 0
]

---

> ğŸ“Œ **InterpretaciÃ³n**
>
> * ( \lambda > 0 ) â†’ inestabilidad
> * ( \lambda < 0 ) â†’ rigidez excesiva
> * ( \lambda \approx 0 ) â†’ rÃ©gimen Ã³ptimo

---

# 6ï¸âƒ£ Protocolo Experimental

### ğŸ§ª Programa 1 â€” Estabilidad bajo no estacionariedad

* Ventanas deslizantes 5â€“10 s
* Tareas cognitivas variables
* Seguimiento de CP

---

### ğŸ§ª Programa 2 â€” PerturbaciÃ³n controlada

* EstÃ­mulos impredecibles
* MediciÃ³n de tiempo de recuperaciÃ³n

---

### ğŸ§ª Programa 3 â€” Transferencia de InformaciÃ³n

Basado en entropÃ­a de transferencia:

[
TE_{EEGâ†’Model}
]

---

# 7ï¸âƒ£ Notebooks Reproducibles

ğŸ“‚ `/notebooks/`

* `01_preprocessing.ipynb`
* `02_encoder_training.ipynb`
* `03_online_adaptation.ipynb`
* `04_critical_regime_analysis.ipynb`

ğŸ”— EjecuciÃ³n recomendada en:

* Google Colab
* Paperspace
* Local GPU

---

# 8ï¸âƒ£ Referencias

<details>
<summary>ğŸ“˜ Karl Friston â€” Free Energy Principle</summary>

Friston, K. (2010). The free-energy principle: a unified brain theory?
DOI: [https://doi.org/10.1038/nrn2787](https://doi.org/10.1038/nrn2787)

FormalizaciÃ³n de minimizaciÃ³n de sorpresa y ajuste predictivo cerebral.

</details>

<details>
<summary>ğŸ“˜ Thomas Schreiber â€” Transfer Entropy</summary>

Schreiber, T. (2000). Measuring information transfer.
DOI: [https://doi.org/10.1103/PhysRevLett.85.461](https://doi.org/10.1103/PhysRevLett.85.461)

IntroducciÃ³n de entropÃ­a de transferencia para sistemas dinÃ¡micos.

</details>

<details>
<summary>ğŸ“˜ Dieter Plenz â€” Neuronal Avalanches</summary>

Beggs & Plenz (2003). Neuronal avalanches in neocortical circuits.
DOI: [https://doi.org/10.1523/JNEUROSCI.23-35-11167.2003](https://doi.org/10.1523/JNEUROSCI.23-35-11167.2003)

Evidencia de criticidad neuronal.

</details>

---

# ğŸ“Œ Resumen Ejecutivo

* La coherencia predictiva se define como reducciÃ³n estructurada de entropÃ­a condicional.
* El modelo debe ser generativo y adaptativo.
* La tasa de aprendizaje depende dinÃ¡micamente de la caÃ­da de coherencia.
* El embedding debe mantenerse en rÃ©gimen crÃ­tico.
* Se incorpora regularizaciÃ³n tipo EWC para evitar deriva catastrÃ³fica.
* La validaciÃ³n requiere anÃ¡lisis espectral, informacional y geomÃ©trico.

---
