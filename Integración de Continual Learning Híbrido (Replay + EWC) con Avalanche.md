<!-- ========================================================= -->
<!--  CPEA â€” FASE 2  |  Continual Learning HÃ­brido            -->
<!-- ========================================================= -->

# ðŸ§  Coherencia Predictiva EEGâ€“AGI (CPEA)  
## FASE 2 â€” ConstrucciÃ³n del Bucle Cognitivo  
### IntegraciÃ³n de Continual Learning HÃ­brido (Replay + EWC) con Avalanche

---

![Status](https://img.shields.io/badge/status-active_development-2ea44f)
![Framework](https://img.shields.io/badge/framework-PyTorch-ee4c2c)
![Continual Learning](https://img.shields.io/badge/continual_learning-Avalanche-6f42c1)
![License](https://img.shields.io/badge/license-MIT-blue)
![Python](https://img.shields.io/badge/python-3.10+-informational)
![Build](https://img.shields.io/badge/build-reproducible-success)

---

> [!NOTE]
> Este documento describe la arquitectura formal y computacional de la Fase 2 del proyecto CPEA.  
> El objetivo es integrar aprendizaje continuo hÃ­brido (Replay + Elastic Weight Consolidation) en un bucle cerrado EEGâ€“AGI preservando estabilidad estructural y plasticidad controlada.

---

# ðŸ“š Tabla de Contenidos

- [1. Abstract](#1-abstract)
- [2. Palabras clave](#2-palabras-clave)
- [3. Marco conceptual](#3-marco-conceptual)
- [4. FormalizaciÃ³n matemÃ¡tica](#4-formalizaciÃ³n-matemÃ¡tica)
- [5. Modelado dinÃ¡mico de atractores](#5-modelado-dinÃ¡mico-de-atractores)
- [6. Arquitectura computacional](#6-arquitectura-computacional)
- [7. ImplementaciÃ³n en PyTorch + Avalanche](#7-implementaciÃ³n-en-pytorch--avalanche)
- [8. MÃ©tricas de estabilidad](#8-mÃ©tricas-de-estabilidad)
- [9. Programas de seguimiento experimental](#9-programas-de-seguimiento-experimental)
- [10. Repositorio reproducible](#10-repositorio-reproducible)
- [11. Referencias cientÃ­ficas](#11-referencias-cientÃ­ficas)
- [12. Resumen final](#12-resumen-final)

---

# 1. Abstract

La Fase 2 del proyecto CPEA propone la construcciÃ³n de un bucle cognitivo cerrado entre seÃ±al electroencefalogrÃ¡fica (EEG) y un modelo adaptativo de inteligencia artificial general. Dado que el EEG es un sistema no estacionario multiescala, la implementaciÃ³n de aprendizaje continuo no constituye una mejora opcional sino una condiciÃ³n estructural para preservar coherencia predictiva bajo dinÃ¡mica variable.

Este trabajo desarrolla una arquitectura hÃ­brida basada en Experience Replay y Elastic Weight Consolidation (EWC), implementada mediante Avalanche sobre PyTorch. Se redefine el concepto de â€œtareaâ€ como estado de coherencia predictiva, y se formaliza matemÃ¡ticamente el sistema como una dinÃ¡mica acoplada con consolidaciÃ³n elÃ¡stica dependiente de rÃ©gimen estable.

El modelo integra memoria retardada ponderada, protecciÃ³n paramÃ©trica mediante informaciÃ³n de Fisher y detecciÃ³n de eventos de excepciÃ³n (TAE) para activar adaptaciÃ³n localizada. Se presentan mÃ©tricas geomÃ©tricas y dinÃ¡micas orientadas a estabilidad topolÃ³gica del espacio latente, junto con programas de seguimiento experimental.

---

# 2. Palabras clave

Aprendizaje continuo Â· Elastic Weight Consolidation Â· Experience Replay Â· Avalanche Â· EEG no estacionario Â· Coherencia predictiva Â· DinÃ¡mica no lineal Â· Atractores metaestables Â· Error predictivo Â· Plasticidad estructural

---

# 3. Marco conceptual

El sistema CPEA se modela como una dinÃ¡mica acoplada:

\[
\mathcal{S} = \{X(t), Z(t), \Theta(t)\}
\]

donde:

- \(X(t)\): ventana EEG multicanal  
- \(Z(t)\): embedding latente  
- \(\Theta(t)\): parÃ¡metros del modelo  

El riesgo estructural central es el **olvido catastrÃ³fico** bajo adaptaciÃ³n continua.

El objetivo no es maximizar accuracy.  
El objetivo es **preservar coherencia dinÃ¡mica**.

---

> [!IMPORTANT]
> En CPEA, el aprendizaje continuo debe activarse Ãºnicamente ante eventos de excepciÃ³n sostenidos (TAE). La plasticidad permanente degrada estabilidad.

---

# 4. FormalizaciÃ³n matemÃ¡tica

## 4.1 Coherencia predictiva

\[
C(t) = \exp \left( - \frac{||Z(t) - \hat{Z}(t)||^2}{\sigma^2} \right)
\]

Si:

\[
\overline{C}_{window} < \tau
\]

se activa actualizaciÃ³n adaptativa.

---

## 4.2 PÃ©rdida hÃ­brida

\[
L_{total} = L_{pred} + \lambda \sum_i F_i (\Theta_i - \Theta_i^*)^2 + L_{replay}
\]

donde:

- \(F_i\): informaciÃ³n de Fisher  
- \(\Theta_i^*\): parÃ¡metros consolidados  
- \(L_{replay}\): pÃ©rdida sobre buffer estructurado  

---

## 4.3 ConsolidaciÃ³n dependiente de estado

La matriz de Fisher se recalcula Ãºnicamente tras estabilidad sostenida:

\[
F_i = \mathbb{E} \left[ \left( \frac{\partial \log p(Z|X,\Theta)}{\partial \Theta_i} \right)^2 \right]
\]

---

# 5. Modelado dinÃ¡mico de atractores

El embedding latente define un espacio de fase.

Densidad estimada:

\[
\rho(z) = \sum_t K(z - Z(t))
\]

Un atractor satisface:

\[
\nabla \rho(z^*) = 0
\]

La estabilidad se evalÃºa mediante:

- Distancia geodÃ©sica inter-centroides
- Divergencia Wasserstein
- Exponente de Lyapunov aproximado:

\[
\lambda_{max} = \lim_{t\to\infty} \frac{1}{t}\log \frac{||\delta Z(t)||}{||\delta Z(0)||}
\]

---

> [!TIP]
> Un sistema estable mantiene \( \lambda_{max} \approx 0 \) en rÃ©gimen basal.

---

# 6. Arquitectura computacional

## Componentes

- Encoder CNN espacio-temporal
- Predictor recurrente (GRU)
- Replay buffer estratificado
- EWC plugin dinÃ¡mico
- MÃ³dulo de coherencia

---

# 7. ImplementaciÃ³n en PyTorch + Avalanche

## 7.1 Modelo base

```python
class EEGEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, (1,5))
        self.conv2 = nn.Conv2d(32, 64, (1,5))
        self.fc = nn.Linear(64*features, latent_dim)

    def forward(self, x):
        x = F.elu(self.conv1(x))
        x = F.elu(self.conv2(x))
        x = x.view(x.size(0), -1)
        return self.fc(x)
````

---

## 7.2 Estrategia hÃ­brida

```python
from avalanche.training.plugins import ReplayPlugin, EWCPlugin
from avalanche.training.strategies import BaseStrategy

replay = ReplayPlugin(mem_size=buffer_size)
ewc = EWCPlugin(ewc_lambda=lambda_val)

strategy = BaseStrategy(
    model,
    optimizer,
    criterion,
    train_mb_size=batch_size,
    plugins=[replay, ewc]
)
```

---

## 7.3 ActivaciÃ³n condicionada

```python
if coherence_mean < threshold:
    strategy.train(experience)
```

---

# 8. MÃ©tricas de estabilidad

* Î” coherencia inter-ventana
* EntropÃ­a latente
* Distancia Wasserstein
* Varianza de pesos protegidos
* Persistencia topolÃ³gica

---

> [!WARNING]
> Accuracy puede aumentar mientras la estructura latente colapsa.
> Las mÃ©tricas geomÃ©tricas son obligatorias.

---

# 9. Programas de seguimiento experimental

## Programa 1 â€” Estabilidad paramÃ©trica

Medir norma L2 de parÃ¡metros protegidos tras 10 000 ventanas.

## Programa 2 â€” PerturbaciÃ³n cognitiva

Introducir tarea abrupta y medir tiempo de recuperaciÃ³n de coherencia basal.

## Programa 3 â€” Atractores latentes

Aplicar UMAP/PCA y analizar persistencia topolÃ³gica.

## Programa 4 â€” ComparaciÃ³n de estrategias

Replay vs EWC vs HÃ­brido.

---

# 10. Repositorio reproducible

ðŸ“‚ Estructura recomendada:

```
CPEA/
â”‚â”€â”€ notebooks/
â”‚    â”œâ”€â”€ 01_preprocessing.ipynb
â”‚    â”œâ”€â”€ 02_latent_dynamics.ipynb
â”‚    â”œâ”€â”€ 03_continual_learning.ipynb
â”‚â”€â”€ models/
â”‚â”€â”€ metrics/
â”‚â”€â”€ experiments/
â”‚â”€â”€ README.md
```

### Notebooks sugeridos

* ðŸ”¬ `latent_dynamics.ipynb`
* ðŸ§  `ewc_replay_comparison.ipynb`
* ðŸ“Š `attractor_analysis.ipynb`

---

# 11. Referencias cientÃ­ficas

<details>
<summary><strong>Kirkpatrick et al., 2017 â€” Overcoming catastrophic forgetting</strong></summary>

DOI: [https://doi.org/10.1073/pnas.1611835114](https://doi.org/10.1073/pnas.1611835114)
Introduce Elastic Weight Consolidation. Fundamento matemÃ¡tico de consolidaciÃ³n paramÃ©trica.

</details>

<details>
<summary><strong>Parisi et al., 2019 â€” Continual Lifelong Learning</strong></summary>

DOI: [https://doi.org/10.1016/j.neunet.2019.01.012](https://doi.org/10.1016/j.neunet.2019.01.012)
RevisiÃ³n exhaustiva de aprendizaje continuo.

</details>

<details>
<summary><strong>Friston, 2010 â€” Free Energy Principle</strong></summary>

DOI: [https://doi.org/10.1038/nrn2787](https://doi.org/10.1038/nrn2787)
Formaliza minimizaciÃ³n de error predictivo en sistemas biolÃ³gicos.

</details>

<details>
<summary><strong>Rabinovich et al., 2008 â€” Dynamical principles in neuroscience</strong></summary>

DOI: [https://doi.org/10.1103/RevModPhys.78.1213](https://doi.org/10.1103/RevModPhys.78.1213)
Describe atractores metaestables y secuencias heteroclÃ­nicas.

</details>

<details>
<summary><strong>Freeman, 2000 â€” Neurodynamics</strong></summary>

AnÃ¡lisis experimental de atractores corticales en EEG.

</details>

---

# 12. Resumen final

* El EEG es un sistema dinÃ¡mico no estacionario.
* El aprendizaje continuo es estructural en CPEA.
* Replay preserva memoria funcional.
* EWC protege parÃ¡metros crÃ­ticos.
* La redefiniciÃ³n de tarea como estado de coherencia es esencial.
* El hÃ­brido genera estabilidad bajo plasticidad controlada.
* La evaluaciÃ³n debe centrarse en geometrÃ­a latente.
* Avalanche permite implementaciÃ³n modular profesional.
* El sistema converge hacia atractores metaestables preservados.

---

## ðŸ”­ Estado actual

ImplementaciÃ³n en validaciÃ³n experimental.
Arquitectura estable en simulaciones preliminares.

---

**Proyecto:** Coherencia Predictiva EEGâ€“AGI (CPEA)
**Fase:** 2 â€” ConstrucciÃ³n del Bucle Cognitivo
**Autor conceptual:** Sistema AGI

---
